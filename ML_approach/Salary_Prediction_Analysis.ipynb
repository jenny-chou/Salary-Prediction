{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary Prediction\n",
    "This project predicts salary based on job descriptions.\n",
    "\n",
    "## Features:\n",
    "1. jobId\n",
    "2. companyId\n",
    "3. jobType\n",
    "4. degree\n",
    "5. major\n",
    "6. industry\n",
    "7. yearsExperience\n",
    "8. milesFromMetropolis\n",
    "9. salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "This project uses machine learning approach, so lots of sklearn classes are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set display to 3 decimal max\n",
    "pd.set_option('precision', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "First thing first, unzip it and see what's inside!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfile_path = os.path.join(\"SalaryPredictions.zip\")\n",
    "zipfile_ref = zipfile.ZipFile(zipfile_path, 'r')\n",
    "zipfile_ref.extractall()\n",
    "zipfile_ref.close()\n",
    "\n",
    "print(os.listdir(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(os.path.join(\"data\", \"test_features.csv\"), header=0, index_col=0)\n",
    "train_df = pd.read_csv(os.path.join(\"data\", \"train_features.csv\"), header=0, index_col=0)\n",
    "train_df['salary'] = pd.read_csv(os.path.join(\"data\", \"train_salaries.csv\"), header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.info())\n",
    "display(train_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.info())\n",
    "display(train_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "Before we dive in to data analysis, we should perform basic cleaning. Look for:\n",
    "1. Any missing values (np.None or np.NaN) in entire training and testing set\n",
    "    * from train_df.info() and test_df.info(), there's no null cell\n",
    "2. Any duplicated samples in training set\n",
    "3. Any invalid yearsExperience (yearsExperience <= 0) in training set\n",
    "4. Any invalid milesFromMetropolis (milesFromMetropolis < 0) in training set\n",
    "5. Any invalid salary (salary < 0) in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Any duplicated rows in training set?\", train_df.duplicated().sum())\n",
    "print(\"Any invalid yearsExperience in training set?\", train_df.yearsExperience.lt(0).sum())\n",
    "print(\"Any invalid milesFromMetropolis in training set?\", train_df.milesFromMetropolis.lt(0).sum())\n",
    "print(\"Any invalid salary in training set?\", train_df.salary.le(0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to:\n",
    "1. Drop duplicated jobs in training set\n",
    "2. Drop jobs with salary <= 0. \n",
    "\n",
    "Since we have 1,000,000 samples, we have enough data to handle removing those jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated jobs in training set\n",
    "train_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop jobs with <=0 salary\n",
    "train_df = train_df.drop(train_df.index[train_df.salary.le(0)])\n",
    "\n",
    "train_df_copy = train_df.copy()\n",
    "print(\"Training set now has shape:\", train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data\n",
    "Now that we've done the basic cleaning, let's look at each feature one by one and generate some statistics reports to further analyze our data.\n",
    "### Company ID\n",
    "In real-life, if a company is well-known or an international organization, the salary is likely to be higher than local startups. We can generate a statistics report to find out which company pays more in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(['companyId']).salary.describe().sort_values('mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 63 companies, let's randomly select some companies and see their statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = train_df.companyId.unique()\n",
    "pick_n_companies = 15\n",
    "rand_companies = companies[np.random.randint(1, len(companies), pick_n_companies)]\n",
    "\n",
    "for i in range(pick_n_companies):\n",
    "    if i==0:\n",
    "        groups = train_df[['companyId', 'salary']].loc[train_df.companyId == rand_companies[i], :]\n",
    "    else:\n",
    "        group = train_df[['companyId', 'salary']].loc[train_df.companyId == rand_companies[i], :]\n",
    "        groups = pd.concat([groups, group], axis=0)\n",
    "\n",
    "groups.boxplot(by='companyId')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations on both statistics reports:\n",
    "1. Similar distribution\n",
    "2. Average salary varies only a little\n",
    "3. Similar deviation\n",
    "4. The min, 25%, 50%, and 75% are almost the same between companies\n",
    "\n",
    "The salary distributions are very similar that no company particularly stands out and gives higher or lower wage, and of course no trend or pattern in this feature that relates to salary. Unless there are other features depending on companyId, so far it's not giving me useful info and it's likely to be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Types\n",
    "There are 8 types of job: CEO, CFO, CTO, JANITOR, JUNIOR, MANAGER, SENIOR and VICE_PRESIDENT.\n",
    "\n",
    "It's reasonable to assume the chief positions offers much higher than JANITOR and JUNIOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.groupby(['jobType']).salary.describe().sort_values(by='mean', ascending=False))\n",
    "train_df[['jobType', 'salary']].boxplot(by='jobType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there's a rule to follow:\n",
    "1. Each job type has 124,000~126,000 sample\n",
    "2. The differences in average salary between each job type is the largest among all featuers\n",
    "    * Average salary ranges from 145 to 70\n",
    "3. Both CFO and CTO offer same salary and distribution\n",
    "4. The ranking of average salary is: CEO > CFO = CTO > VP > MANAGER > SENIOR > JUNIOR >> JANITOR\n",
    "5. The deviation also decreases with average salary\n",
    "6. Most positions are slightly positive skewed Gaussian distribution\n",
    "\n",
    "Some data transformation is needed to emphasize the rules in jobType vs. salary. \n",
    "\n",
    "To visualize more on the skewness, use histograms and calculation of skewness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobTypes = train_df.jobType.unique()\n",
    "plt.figure(figsize = (14, 6))\n",
    "\n",
    "for i in range(len(jobTypes)):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.hist(train_df.salary[train_df.jobType == jobTypes[i]])\n",
    "    plt.title(jobTypes[i])\n",
    "plt.show()\n",
    "\n",
    "train_df.groupby(['jobType']).salary.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram and skew table back up our previous assumption of positive skewness. As shown in table, the skewness are all positive and very small, and the histogram for each position only slightly lean toward left. \n",
    "\n",
    "The skewness level is: CEO < CFO = CTO < VICE_PRESIDENT < MANAGER = SENIOR < JUNIOR < JANITOR, which is opposite of average salary ranking! My guessing would be that lower positions usually have more room to negotiate for more salary due to other factors, such as world class company might offer more for the same position than local startups, or people with prior experience in the industry tends to be offered more than university graduates, and so on. And people interviewing for higher position might not be for the money, so the positions tends to offer standard salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree\n",
    "There are 5 types of degree: BACHLORS, DOCTORAL, HIGH_SCHOOL, MASTERS, and NONE\n",
    "\n",
    "I expect that more professional position asks for higher educatation, so jobs requiring DOCTORAL degree might offer much higher than HIGH SCHOOL graduated. But will it be the same for DOCTORAL vs. MASTER degree? How much of a gap in salary will it be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(['degree']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From number of jobs, we see that:\n",
    "1. Number of jobs asking for NONE degree is almost the same as asking for HIGH_SCHOOL degree.\n",
    "2. Number of jobs asking for BACHELORS, MASTERS, and DOCTORAL are almost the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.groupby(['degree']).describe().salary.sort_values(by='mean', ascending=False))\n",
    "train_df[['degree', 'salary']].boxplot(by='degree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From statistics reports, it can be observed that:\n",
    "1. Jobs asking for HIGH_SCHOOL degree offer almost the same salary as not asking any degree (NONE). \n",
    "2. Jobs asking for HIGH_SCHOOL degree or NONE offer much less than BACHELORS and above. \n",
    "3. Distribution for BACHELORS, MASTERS, and DOCTORAL shifts and narrows in same rate.\n",
    "4. The ranking of average salary is: DOCTORAL > MASTERS > BACHELORS >> HIGH_SCHOOL > NONE.\n",
    "5. Except HIGH_SCHOOL, the ranking of deviation is the same as average salary\n",
    "    * HIGH_SCHOOL has highest deviation\n",
    "    * Maybe some jobs are intern positions that allow students to continue higher education while working, thus the salary is negotiable judging from their potential\n",
    "    * Maybe other jobs are general labor jobs that don't require any professional knowledge, thus paid less\n",
    "6. All degree are Gaussian distributed. And maybe positively skewed.\n",
    "\n",
    "Combine with finding from number of jobs, seems like data can be seperated to two groups:\n",
    "1. NONE and HIGH_SCHOOL: more job opening but lower offer\n",
    "2. BACHELORS, MASTERS, DOCTORAL: less job openings and higher offer\n",
    "\n",
    "Will it simplify the degree feature too much? Need to experiments them to find out!\n",
    "\n",
    "Again, let's verify the Gaussian distribution and skewness with histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobTypes = train_df.degree.unique()\n",
    "plt.figure(figsize = (14, 6))\n",
    "\n",
    "for i in range(len(jobTypes)):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.hist(train_df.salary[train_df.degree == jobTypes[i]])\n",
    "    plt.title(jobTypes[i])\n",
    "plt.show()\n",
    "\n",
    "train_df.groupby(['degree']).salary.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it's Gaussian and positive skewed. \n",
    "\n",
    "The level of skewness is: MASTERS = NONE = BACHELORS > DOCTORAL >> HIGH_SCHOOL, all with very little difference. It's interesting that HIGH_SCHOOL degree is skewed the least and also offered second to least, suggesting that jobs requiring HIGH_SCHOOL are more fixed compare to other degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major\n",
    "There are 9 majors: BIOLOGY, BUSINESS, CHEMISTRY, COMPSCI (computer science), ENGINEERING, LITERATURE, MATH, NONE, and PHYSICS\n",
    "\n",
    "I expect the technical (ENGINEERING and COMPSCI) and BUSINESS major might be offered more than pure science (BIOLOGY, CHEMISTRY, MATH, PHYSICS) and LITERATURE major since they're more practical (eg. designing product, analyzing market trend, etc.) and less theoratical or economic (eg. research papers, studying Shakespeare, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(['major']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of jobs asking for no major is more than other jobs combined and also more than half of the data. The imbalance data says that more than half of the offerings don't look at applicant's major, everyone can apply. But this raises some questions:\n",
    "* Does it reflect on salary? \n",
    "* If the major is irrelevent, do those jobs have lower wage because it's not professional? \n",
    "     * For example, workers at warehouse or clark at supermarket might not need be any major to work.\n",
    "* Does it suggest us that the major doesn't provide valueable information since so many jobs don't require any and we should focus on other features such as yearsExperience? \n",
    "* Is it related to other features?\n",
    "    * For example, OLD and SERVICE industry are willing to hire more NONE major employees than WEB or FINANCE?\n",
    "    * For example, is employee likely to be paid more with higher education diploma in any major?\n",
    "\n",
    "Let's confirm it from statistics report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.groupby(['major']).salary.describe().sort_values(by='mean', ascending=False))\n",
    "train_df[['major', 'salary']].boxplot(by='major')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. NONE obviously has lower distribution than others by great amount.\n",
    "2. The ranking of average salary is: ENGINEERING > BUSINESS > MATH > COMPSCI > PHYSICS > CHEMISTRY > BIOLOGY > LITERATURE >> NONE\n",
    "3. BUSINESS and ENGINEERING have larger deviation\n",
    "4. LITERATURE has smallest deviation\n",
    "5. Ranking of deviation is similar to mean, except NONE has higher deviation\n",
    "    * Lower paid jobs usually offers the standard salary\n",
    "    * Higher paid jobs depends more on other factors, thus have wider range of salary\n",
    "    * There can be various jobs that don't require any major, thus it also heavily depends on other features\n",
    "6. Gaussian distributed and skew to positive\n",
    "\n",
    "This verifies our initial guessing that jobs with no major requirement might be less professional and thus paid less. But consider the imbalanced data, it's too early to assume all jobs not requiring major pay less since we're not sure if there are other factors influencing the salary. Need more digging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobTypes = train_df.major.unique()\n",
    "plt.figure(figsize = (14, 6))\n",
    "\n",
    "for i in range(len(jobTypes)):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.hist(train_df.salary[train_df.major == jobTypes[i]])\n",
    "    plt.title(jobTypes[i])\n",
    "plt.show()\n",
    "\n",
    "train_df.groupby(['major']).salary.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUSINESS is the most skewed and BIOLOGY and NONE are least skewed. But compare to other features like jobType, all majors are skewed at similar rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry\n",
    "There are 7 types of industry: AUTO (automobile), EDUCATION, FINANCE, HEALTH, OIL, SERVICE, and WEB\n",
    "\n",
    "I expect FINANCE and WEB pays relatively higher and SERVICE pays lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(['industry']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each industry has around 142,800 openings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.groupby(['industry']).salary.describe().sort_values(by='mean', ascending=False))\n",
    "train_df[['industry', 'salary']].boxplot(by='industry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "1. FINANCE and OIL(!) have highest wage and most stretched deviation\n",
    "2. FINANCE and OIL have the same distribution\n",
    "3. The ranking of average salary is: FINANCE = OIL > WEB > HEALTH > AUTO > SERVICE > EDUCATION\n",
    "4. The ranking of deviation is the same as average salary\n",
    "    * There's a significant drop between WEB and HEALTH\n",
    "    * Is there any relationship between group:(OIL, FINANCE, WEB) and group:(HEALTH, AUTO, SERVICE, EDUCATION)?\n",
    "5. The differences between each industry's average salary are the smallest among all features\n",
    "    * The average salary ranges from 130 to 99\n",
    "5. Gaussian distribution and positive skew\n",
    "\n",
    "Let's look at the salary histograms and skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobTypes = train_df.industry.unique()\n",
    "plt.figure(figsize = (14, 6))\n",
    "\n",
    "for i in range(len(jobTypes)):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.hist(train_df.salary[train_df.industry == jobTypes[i]])\n",
    "    plt.title(jobTypes[i])\n",
    "plt.show()\n",
    "\n",
    "train_df.groupby(['industry']).salary.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skewness is the relatively small among all features and skews at similar rate. Similar to the deviations, there's a gap in skewness between group:(OIL, WEB, FINANCE) and group:(AUTO, HEALTH, SERVICE, EDUCATION)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Years of Experience\n",
    "Span from 0 to 24 years. Let's select a few and see their distributions.\n",
    "\n",
    "I expect the more years of experience a job asks, the more it pays. But is there a plateau? Is the relationship linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_df.yearsExperience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years= np.arange(24+1)\n",
    "for i in range(len(years)):\n",
    "    if i==0:\n",
    "        groups = train_df[['yearsExperience', 'salary']][train_df.yearsExperience == years[i]]\n",
    "    else:\n",
    "        group = train_df[['yearsExperience', 'salary']][train_df.yearsExperience == years[i]]\n",
    "        groups = pd.concat([groups, group], axis=0)\n",
    "\n",
    "groups.boxplot(by='yearsExperience')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected:\n",
    "1. The yearsExperience and average salary have linear relationship\n",
    "2. The deviation increases with average salary\n",
    "3. Gaussian distribution and positive skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years= np.arange(0, 24+1, 3)\n",
    "plt.figure(figsize = (14, 6))\n",
    "\n",
    "for i in range(len(years)):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.hist(train_df.salary[train_df.yearsExperience == years[i]])\n",
    "    plt.title(years[i])\n",
    "plt.show()\n",
    "\n",
    "train_df.groupby(['yearsExperience']).salary.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skewness is the smallest of all features and skews at similar rate. Overall, the skewness increases with yearsExperience.\n",
    "\n",
    "And to show the linear relationship, here's the plot of yearsExperience vs. average salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.groupby(['yearsExperience']).salary.describe())\n",
    "train_df.groupby(['yearsExperience']).salary.describe()['mean'].plot(x='yearsExperience', y='salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's an obvious slope of 2 and offset 90 linear relationship.\n",
    "\n",
    "yearsExperience is an independent feature, Gaussian distributed, and has a simple linear trend pattern. If we scale it down using standard scaler, we won't distored the pattern and it'll release some burdent in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miles from metropolis\n",
    "Span from 0 to 99 miles. Like yearsExperience, let's select a few and see their distributions.\n",
    "\n",
    "I expect companies having offices in metropolis are likely to have higher net worth, thus the closer to metropolis, the more it pays. Again, is there a plateau? Is the relationship linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_df.milesFromMetropolis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miles= np.arange(0, 99+1, 4)\n",
    "for i in range(len(miles)):\n",
    "    if i==0:\n",
    "        groups = train_df[['milesFromMetropolis', 'salary']][train_df.milesFromMetropolis == miles[i]]\n",
    "    else:\n",
    "        group = train_df[['milesFromMetropolis', 'salary']][train_df.milesFromMetropolis == miles[i]]\n",
    "        groups = pd.concat([groups, group], axis=0)\n",
    "\n",
    "groups.boxplot(by='milesFromMetropolis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It's a inverse linear relationship\n",
    "2. Deviation decreases with average salary\n",
    "3. Gaussian distribution and positive skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miles= np.arange(0, 99+1, 12)\n",
    "plt.figure(figsize = (14, 6))\n",
    "\n",
    "for i in range(len(miles)):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.hist(train_df.salary[train_df.milesFromMetropolis == miles[i]])\n",
    "    plt.title(miles[i])\n",
    "plt.show()\n",
    "\n",
    "train_df.groupby(['milesFromMetropolis']).salary.skew()[miles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skewness is the small among all features and skews at similar rate. There's no obvious relationship between skewness and and milesFromMetropolis.\n",
    "\n",
    "And to show the linearity, here's the plot of milesFromMetropolis vs. average salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.milesFromMetropolis.describe())\n",
    "train_df.groupby(['milesFromMetropolis']).salary.describe()['mean'].plot(x='milesFromMetropolis', y='salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's an obvious slope of -0.4 and offset 135 linear trend.\n",
    "\n",
    "Similar to yearsExperience, milesFromMetropolis is an independent feature, Gaussian distributed, and has a simple (reversed) linear trend pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis conclusion\n",
    "1. Without further data transformation, all features have obvious pattern/ranking\n",
    "2. All features are Gaussian distributed and positive skewness\n",
    "3. Higher average salary often pairs with wider distribution (larger deviation)\n",
    "4. **companyId** provides little to no information related to salary\n",
    "5. **jobType** has highest variance\n",
    "    * Avearage salary ranges from 145 to 70\n",
    "6. **degree** can potentially be split to two groups\n",
    "    * NONE and HIGH_SCHOOL: more job opening but lower offer\n",
    "    * BACHELORS, MASTERS, DOCTORAL: less job openings and higher offer\n",
    "7. **major** has imbalanced data\n",
    "    * More than half of dataset don't require a major\n",
    "8. **industry** has smallest variance\n",
    "    * Average salary ranges from 130 to 99\n",
    "9. **yearsExperience** and **milesFromMetropolis** both are independent and linearly related to salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data\n",
    "The goal is to transform our data according to certain rules to further reveal the hidden pattern.\n",
    "\n",
    "Obviously, we have to transform strings to meaningful numerical values. companyId, jobType, degree, major, and industry are the columns in dataset that need convertion. We have three options:\n",
    "1. One-Hot Encoder\n",
    "2. Label Encoder\n",
    "3. Custome Label Encoder\n",
    "\n",
    "Let's try all three and see which method gives me features that are:\n",
    "1. Independent\n",
    "2. Most correlated to salary\n",
    "\n",
    "### Outliers\n",
    "\n",
    "### Encoders\n",
    "To find the best encoding method, let's use 50,000 samples for testing and see which yields best correlated features.\n",
    "#### One Hot Encoder\n",
    "In this case, one-hot encoding 50,000 samples takes too long, so use 500 samples instead. Also show first 40 out of 95 correlations with salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoders\n",
    "To find the best encoding method, let's use 50,000 samples for testing and see which yields best correlated features.\n",
    "#### One Hot Encoder\n",
    "In this case, one-hot encoding 50,000 samples takes too long, so use 500 samples instead. Also show first 40 out of 95 correlations with salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a class instead: class OneHotEncodingFeatures:\n",
    "def one_hot_encoder(train, features):\n",
    "    for feature in features:\n",
    "        categories = train[feature].unique()\n",
    "        for category in categories:\n",
    "            train[feature + \"_\" + category] = (train[feature] == category).astype(int)\n",
    "        train = train.drop(columns=feature)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# because one-hot encoding 50,000 samples takes too long, use 5,000 samples instead\n",
    "example_set = train_df[:5000].copy()\n",
    "display(one_hot_encoder(example_set, ['jobType', 'degree', 'major', 'industry', 'companyId']).head(5))\n",
    "\n",
    "show_corr = 40\n",
    "one_hot_encoder(example_set, ['jobType', 'degree', 'major', 'industry', 'companyId']).corr().salary.iloc[:show_corr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot encoding is the standard way to transform strings to numericals, but this gives too many features (95 columns) and takes too long to generate. And if we look into the correlation matrix, the maximum negative correlation is jobType_JANITOR with magnitude of 0.457 and positive correlation is 0.368 from yearsExperience, which didn't need to be one-hot encoded. Thus, one-hot encoding doesn't give me strong correlated features.\n",
    "\n",
    "#### Label Encoder\n",
    "This method uses sklearn.preprocessing.LabelEncoder() to assign numerical values starting from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a class instead: class LabelEncodingFeatures:\n",
    "def label_encoder(train, features, test=None):\n",
    "    encoder = sklearn.preprocessing.LabelEncoder()\n",
    "    for feature in features:\n",
    "        train[feature] = encoder.fit_transform(train[feature])\n",
    "        if test:\n",
    "            test[feature] = encoder.transform(test[feature])\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_set = train_df[:50000].copy()\n",
    "features = ['jobType', 'degree', 'major', 'industry', 'companyId']\n",
    "\n",
    "display(label_encoder(example_set, features).head(5))\n",
    "\n",
    "correlation = label_encoder(example_set, features).corr()\n",
    "print(correlation['salary'])\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(correlation, annot=True, vmin=-1, cmap=\"RdBu_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Label Encoder:\n",
    "1. No new features\n",
    "2. Except companyId and industry, other features have good correlation with salary\n",
    "3. Except major and degree, other pairs of features are independent from each other\n",
    "\n",
    "This approach looks promissing! Let's see if we can lift the perfomance up a notch with custom Laber Encoder.\n",
    "\n",
    "#### Custom Label Encoder\n",
    "This method uses the ranking of average salary to assign numerical values starting from 1.\n",
    "* Starts from 1 because rank() starts from 1, and I don't want to \"deactivate\" any label of each feature with 0\n",
    "* Average salaries are rounded down, so labels with same average salary will have the same rank\n",
    "* companyId: 63 companies are ranked according to average salary\n",
    "* **jobType**: CEO = 1, CFO = 2, CTO = 2, VP = 4, MANAGER = 5, SENIOR = 6, JUNIOR = 7, JANITOR = 8\n",
    "* **degree**: DOCTORAL = 1, MASTERS = 2, BACHELORS = 3, HIGH_SCHOOL = 4, NONE = 5\n",
    "* **major**: ENGINEERING = 1, BUSINESS = 2, MATH = 3, COMPSCI = 4, PHYSICS = 5, CHEMISTRY = 6, BIOLOGY = 7, LITERATURE = 8, NONE = 9\n",
    "* **industry**: FINANCE = 1, OIL = 1, WEB = 3, HEALTH = 4, AUTO = 5, SERVICE = 6, EDUCATION = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a class instead: class RankFeatures:\n",
    "def mean_salary_encoder(train, features, test=None):\n",
    "    for feature in features:\n",
    "        salary_dict = dict(train.groupby([feature]).salary.mean())\n",
    "        train[feature] = train[feature].map(salary_dict)\n",
    "        if test:\n",
    "            test[feature] = test[feature].map(salary_dict)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_set = train_df[:50000].copy()\n",
    "features = ['jobType', 'degree', 'major', 'industry', 'companyId']\n",
    "\n",
    "display(mean_salary_encoder(example_set, features)[0].head(5))\n",
    "\n",
    "correlation = mean_salary_encoder(example_set, features)[0].corr()\n",
    "print(correlation.salary)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(correlation, annot=True, vmin=-1, cmap=\"RdBu_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From correlation matrix, we observe:\n",
    "1. Correlation matrix of custom Label Encoder looks like correlation matrix of Label Encoder but better\n",
    "2. Between Salary and other features, only companyId has low correlation with salary\n",
    "    * Consider removing companyId\n",
    "3. Between feature to feature, Major and Industry are highly correlated.\n",
    "    * Consider merging Major and Industry to avoid possible noise\n",
    "4. Between two features, pairs other than Major-Industry have low correlation, meaning two features are relatively independent from each other\n",
    "\n",
    "Thus makes it the plausible encoder for our case.\n",
    "\n",
    "### Transform data and test performance\n",
    "Now we've selected our encoder, next is to transform data. From the correlation matrix with custom label encoder, we see some potential options for feature selection:\n",
    "1. Drop companyId\n",
    "2. Merge major or degree\n",
    "\n",
    "We'll perform a quick test with LinearRegression model to see which of the following data transformaiton method yields the least MSE:\n",
    "1. Keep companyId and not merge degree and major\n",
    "2. Drop companyId and not merge degree and major\n",
    "3. Keep companyId and merge degree and major\n",
    "4. Drop companyId and merge degree and major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_k_columns(train, features, col_name=\"group\", test=None):\n",
    "    group = train.groupby(features).salary\n",
    "    Q1 = group.quantile(0.25)\n",
    "    Q3 = group.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "\n",
    "    salary_dictionaries = []\n",
    "#     salary_dictionaries.append(dict(lower_bound,    name = col_names + \"group_lower\"))\n",
    "    salary_dictionaries.append(dict(group.min(),    name = col_name + \"group_min\"))\n",
    "    salary_dictionaries.append(dict(Q1,             name = col_name + \"group_Q1\"))\n",
    "    salary_dictionaries.append(dict(group.mean(),   name = col_name + \"group_mean\"))\n",
    "    salary_dictionaries.append(dict(group.median(), name = col_name + \"group_median\"))\n",
    "    salary_dictionaries.append(dict(Q3,             name = col_name + \"group_Q3\"))\n",
    "#     salary_dictionaries.append(dict(upper_bound,    name = col_names + \"group_upper\"))\n",
    "    salary_dictionaries.append(dict(group.max(),    name = col_names + \"group_max\"))\n",
    "    \n",
    "    for dictionary in salary_dictionaries:\n",
    "        salary = []\n",
    "        for index in train.index:\n",
    "            combination = []\n",
    "            for i in range(len(features)):\n",
    "                combination.append(train.loc[index, features[i]])\n",
    "            salary.append(dictionary[tuple(combination)])\n",
    "        train[dictionary['name']] = pd.Series(salary, index=train.index)\n",
    "\n",
    "        if test:\n",
    "            salary = []\n",
    "            for index in test.index:\n",
    "                combination = []\n",
    "                for i in range(len(features)):\n",
    "                    combination.append(test.loc[index, features[i]])\n",
    "                salary.append(dictionary[tuple(combination)])\n",
    "            test[dictionary['name']] = pd.Series(salary, index=test.index)\n",
    "        \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(train, features, col_name, test=None):\n",
    "    train, test = group_k_columns(train, features, col_name, test)\n",
    "    train, test = mean_salary_encoder(train, features, test)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['companyId', 'jobType', 'degree', 'major', 'industry']\n",
    "stat_col_name = 'CJDMI'\n",
    "correlation = feature_engineer(train_df.deepcopy(), features, col_name)[0].corr()\n",
    "\n",
    "print(correlation.salary)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(correlation, annot=True, vmin=-1, cmap=\"RdBu_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models\n",
    "### Baseline model\n",
    "Create a naive model and measure its efficiency as my performance metrics baseline. I'm using average salary for each industry as my baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_dict = dict(train_df.groupby(['industry']).describe().salary.mean())\n",
    "baseline_true = train_df.salary.values.astype(float)\n",
    "baseline_pred = train.industry.map(industry_dict)\n",
    "\n",
    "mse = mean_squared_error(baseline_true, baseline_pred)\n",
    "mae = mean_absolute_error(baseline_true, baseline_pred)\n",
    "print(\"Baseline: MSE=%.3f\\tMAE=%.3f\" % (mse, mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression and ensamble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = ['companyId', 'jobType', 'degree', 'major', 'industry']\n",
    "stat_col_name = 'CJDMI'\n",
    "train = feature_engineer(train_df.deepcopy(), features, col_name)[0]\n",
    "\n",
    "model_list = [('LR', LinearRegression()), \n",
    "              ('LASSO', Lasso()), \n",
    "              ('EN', ElasticNet()), \n",
    "              ('CART', DecisionTreeRegressor()),\n",
    "              (\"RF\", RandomForestRegressor()),\n",
    "              ## Boosting\n",
    "              ('AB', AdaBoostRegressor()), \n",
    "              ('GBR', GradientBoostingRegressor()), \n",
    "              ## Bagging\n",
    "              ('RF', RandomForestRegressor(n_estimators=60, max_depth=15, min_samples_split=80, max_features=8)), \n",
    "              ('ET', ExtraTreesRegressor())]\n",
    "\n",
    "for name, model in model_list:\n",
    "    scores = sklearn.model_selection.cross_val_score(model, train.drop(columns='salary'), train.salary, scoring='neg_mean_squared_error')\n",
    "    mean_mse.append(-1.0 * np.mean(scores))\n",
    "    print(\"%s:\\tMSE=%.3f\" % (name, -1.0 * np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick models that have lower MSE: LinearRegression, Lasso, ElasticNet, and GradientBoostingRegressor. Tune hyperparameters and try other adjustments to improve performance.\n",
    "\n",
    "### Scalng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feature_engineer(train_df.deepcopy(), ['companyId', 'jobType', 'degree', 'major', 'industry'], 'CJDMI')[0]\n",
    "\n",
    "model_list = [(\"scaledLR\",    Pipeline([(\"Scaler\", StandardScaler()), (\"LR\", LinearRegression())])),\n",
    "              (\"scaledLASSO\", Pipeline([(\"Scaler\", StandardScaler()), (\"LASSO\", Lasso())])),\n",
    "              (\"scaledEN\",    Pipeline([(\"Scaler\", StandardScaler()), (\"EN\", ElasticNet())])),\n",
    "              (\"scaledGBR\",   Pipeline([(\"Scaler\", StandardScaler()), (\"GBR\", GradientBoostingRegressor(max_depth=6, loss='huber'))]))]\n",
    "\n",
    "for name, model in model_list:\n",
    "    scores = sklearn.model_selection.cross_val_score(model, train.drop(columns='salary'), train.salary, scoring='neg_mean_squared_error')\n",
    "    print(\"%s:\\tMSE=%.3f\" % (name, -1.0 * np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data doesn't improve the performance. Let's try tuning hyperparameters.\n",
    "\n",
    "### Tuning\n",
    "#### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = feature_engineer(train_df.deepcopy(), ['companyId', 'jobType', 'degree', 'major', 'industry'], 'CJDMI')[0]\n",
    "\n",
    "for n_est in [50, 100, 200]:\n",
    "    for loss in ['ls', 'huber']:\n",
    "        for depth in [6, 8, 10]:\n",
    "            model = GradientBoostingRegressor(n_estimators=100, loss='ls', max_depth=8)\n",
    "            scores = sklearn.model_selection.cross_val_score(model, train.drop(columns='salary'), train.salary, scoring='neg_mean_squared_error')\n",
    "            mse = -1.0 * np.mean(scores)\n",
    "            print(\"(%d, %s, %.3f):\\tMSE=%.3f\" % (n_est, 'ls', depth, mse))\n",
    "\n",
    "            model.fit(train.drop(columns='salary'), train.salary)\n",
    "            feature_importance = model.feature_importances_\n",
    "            for i in range(len(feature_importance)):\n",
    "                print(\"%s  %.6f\" % (train.drop(columns='salary').columns[i], feature_importance[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feature_engineer(train_df.deepcopy(), ['companyId', 'jobType', 'degree', 'major', 'industry'], 'CJDMI')[0]\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100, loss='ls', max_depth=8)\n",
    "scores = sklearn.model_selection.cross_val_score(model, train.drop(columns='salary'), train.salary, scoring='neg_mean_squared_error')\n",
    "mse = -1.0 * np.mean(scores)\n",
    "\n",
    "model.fit(train.drop(columns='salary'), train.salary)\n",
    "feature_importance = model.feature_importances_\n",
    "for i in range(len(feature_importance)):\n",
    "    print(\"%s  %.6f\" % (train.drop(columns='salary').columns[i], feature_importance[i]))\n",
    "\n",
    "predictions = model.predict(test_features).round(3)\n",
    "test_salary = pd.DataFrame(predictions, index=test_features.index)\n",
    "test_salary.to_csv(os.path.join(\"data\", \"test_salary_prediction.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = group_k_columns(train, 2, ['degree', 'major'], \"DM\")\n",
    "train = group_k_columns(train, 2, ['jobType', 'degree'], \"JD\")\n",
    "train = group_k_columns(train, 2, ['jobType', 'major'], \"JM\")\n",
    "train = group_k_columns(train, 3, ['jobType', 'degree', 'major'], \"JDM\")\n",
    "train = group_k_columns(train, 5, ['companyId', 'jobType', 'degree', 'major', 'industry'], 'CJDMI')\n",
    "# train = group_k_columns(train, 7, ['companyId', 'jobType', 'degree', 'major', 'industry', 'yearsExperience', 'milesFromMetropolis'], 'CJDMIYM')\n",
    "train = mean_salary_encoder(train, ['companyId', 'jobType', 'degree', 'major', 'industry', 'yearsExperience', 'milesFromMetropolis'])\n",
    "    \n",
    "(50, ls, 6.000):\tMSE=334.113\n",
    "(50, ls, 8.000):\tMSE=328.809\n",
    "(50, ls, 10.000):\tMSE=327.740\n",
    "(50, huber, 6.000):\tMSE=334.282\n",
    "(50, huber, 8.000):\tMSE=328.878\n",
    "    \n",
    "\n",
    "    \n",
    "salary_mean_dict = dict(train.groupby([feature]).salary.mean())\n",
    "# salary_mean_dict = dict(np.floor(train.groupby([feature]).salary.mean()).rank())\n",
    "    \n",
    "train = group_k_columns(train, 5, ['companyId', 'jobType', 'degree', 'major', 'industry'], 'CJDMI')\n",
    "train = mean_salary_encoder(train, ['companyId', 'jobType', 'degree', 'major', 'industry'])\n",
    "(50, ls, 12.000):\tMSE=332.343\n",
    "(150, ls, 8.000):\tMSE=327.946\n",
    "    \n",
    "companyId  0.000700\n",
    "jobType  0.005244\n",
    "degree  0.003090\n",
    "major  0.001900\n",
    "industry  0.003331\n",
    "yearsExperience  0.155236\n",
    "milesFromMetropolis  0.107955\n",
    "CJDMI_mean  0.707699\n",
    "CJDMI_median  0.014845\n",
    "\n",
    "\n",
    "\n",
    "# salary_mean_dict = dict(train.groupby([feature]).salary.mean())\n",
    "salary_mean_dict = dict(np.floor(train.groupby([feature]).salary.mean()).rank())\n",
    "train = group_k_columns(train, 3, ['degree', 'major', 'industry'], \"DMI\")\n",
    "train = group_k_columns(train, 4, ['companyId', 'degree', 'major', 'industry'], \"CDMI\")\n",
    "train = group_k_columns(train, 5, ['companyId', 'jobType', 'degree', 'major', 'industry'], 'CJDMI')\n",
    "train = mean_salary_encoder(train, ['companyId', 'jobType', 'degree', 'major', 'industry'])\n",
    "\n",
    "(150, ls, 8.000):\tMSE=327.145\n",
    "companyId  0.000053\n",
    "jobType  0.006356\n",
    "degree  0.001235\n",
    "major  0.000139\n",
    "industry  0.000722\n",
    "yearsExperience  0.155082\n",
    "milesFromMetropolis  0.107684\n",
    "DMI_mean  0.004945\n",
    "DMI_median  0.001223\n",
    "CDMI_mean  0.001024\n",
    "CDMI_median  0.001079\n",
    "CJDMI_mean  0.706410\n",
    "CJDMI_median  0.014051\n",
    "\n",
    "\n",
    "\n",
    "# salary_dict = dict(train.groupby([feature]).salary.mean())\n",
    "salary_dict = dict(np.floor(train.groupby([feature]).salary.mean()).rank())\n",
    "\n",
    "train = group_k_columns(train, 5, ['companyId', 'jobType', 'degree', 'major', 'industry'], 'CJDMI')\n",
    "train = mean_salary_encoder(train, ['companyId', 'jobType', 'degree', 'major', 'industry'])\n",
    "\n",
    "salary_mean_dict = dict(train.groupby(col_names).salary.mean())\n",
    "salary_mean_dict['name'] = new_col_name_mean\n",
    "salary_Q0_dict = dict(train.groupby(col_names).salary.quantile(0))\n",
    "salary_Q0_dict['name'] = new_col_name_Q0\n",
    "salary_Q1_dict = dict(train.groupby(col_names).salary.quantile(0.25))\n",
    "salary_Q1_dict['name'] = new_col_name_Q1\n",
    "salary_Q2_dict = dict(train.groupby(col_names).salary.median())\n",
    "salary_Q2_dict['name'] = new_col_name_Q2\n",
    "salary_Q3_dict = dict(train.groupby(col_names).salary.quantile(0.75))\n",
    "salary_Q3_dict['name'] = new_col_name_Q3\n",
    "salary_Q4_dict = dict(train.groupby(col_names).salary.quantile(1))\n",
    "salary_Q4_dict['name'] = new_col_name_Q4\n",
    "salary_dictionaries = [salary_mean_dict, salary_Q0_dict, salary_Q1_dict, salary_Q2_dict, salary_Q3_dict, salary_Q4_dict]\n",
    "\n",
    "(100, ls, 8.000):\tMSE=306.777\n",
    "companyId  0.000021\n",
    "jobType  0.003827\n",
    "degree  0.002999\n",
    "major  0.001062\n",
    "industry  0.002066\n",
    "yearsExperience  0.152040\n",
    "milesFromMetropolis  0.104977\n",
    "CJDMI_mean  0.658974\n",
    "CJDMI_Q0  0.009351\n",
    "CJDMI_Q1  0.031602\n",
    "CJDMI_Q2  0.006909\n",
    "CJDMI_Q3  0.008939\n",
    "CJDMI_Q4  0.017233\n",
    "\n",
    "\n",
    "\n",
    "salary_dict = dict(train.groupby([feature]).salary.mean())\n",
    "# salary_dict = dict(np.floor(train.groupby([feature]).salary.mean()).rank())\n",
    "\n",
    "train = group_k_columns(train, 3, ['degree', 'major', 'industry'], 'DMI')\n",
    "train = mean_salary_encoder(train, ['jobType'])\n",
    "train = train.drop(columns=['companyId', 'degree', 'major', 'industry'])\n",
    "\n",
    "(100, ls, 8.000):\tMSE=356.407\n",
    "jobType  0.415251\n",
    "degree  0.000361\n",
    "major  0.000179\n",
    "industry  0.002465\n",
    "yearsExperience  0.186674\n",
    "milesFromMetropolis  0.129748\n",
    "DMI_mean  0.130898\n",
    "DMI_Q0  0.002157\n",
    "DMI_Q1  0.008558\n",
    "DMI_Q2  0.022342\n",
    "DMI_Q3  0.087371\n",
    "DMI_Q4  0.013997\n",
    "\n",
    "\n",
    "\n",
    "salary_mean_dict = dict(train.groupby(col_names).salary.mean())\n",
    "salary_mean_dict['name'] = new_col_name_mean\n",
    "# salary_Q0_dict = dict(train.groupby(col_names).salary.quantile(0))\n",
    "# salary_Q0_dict['name'] = new_col_name_Q0\n",
    "# salary_Q1_dict = dict(train.groupby(col_names).salary.quantile(0.25))\n",
    "# salary_Q1_dict['name'] = new_col_name_Q1\n",
    "# salary_Q2_dict = dict(train.groupby(col_names).salary.median())\n",
    "# salary_Q2_dict['name'] = new_col_name_Q2\n",
    "# salary_Q3_dict = dict(train.groupby(col_names).salary.quantile(0.75))\n",
    "# salary_Q3_dict['name'] = new_col_name_Q3\n",
    "# salary_Q4_dict = dict(train.groupby(col_names).salary.quantile(1))\n",
    "# salary_Q4_dict['name'] = new_col_name_Q4\n",
    "\n",
    "(100, ls, 8.000):\tMSE=356.385\n",
    "jobType  0.414720\n",
    "degree  0.000792\n",
    "major  0.000316\n",
    "industry  0.007666\n",
    "yearsExperience  0.186760\n",
    "milesFromMetropolis  0.129649\n",
    "DM_mean  0.260096\n",
    "\n",
    "\n",
    "\n",
    "# salary_dict = dict(train.groupby([feature]).salary.mean())\n",
    "salary_dict = dict(np.floor(train.groupby([feature]).salary.mean()).rank())\n",
    "\n",
    "(100, ls, 8.000):\tMSE=356.357\n",
    "jobType  0.414795\n",
    "degree  0.000755\n",
    "major  0.000276\n",
    "industry  0.007868\n",
    "yearsExperience  0.186807\n",
    "milesFromMetropolis  0.129701\n",
    "DM_mean  0.259799\n",
    "\n",
    "\n",
    "\n",
    "train = group_k_columns(train, 3, ['jobType', 'yearsExperience', 'milesFromMetropolis'], 'JYM')\n",
    "train = mean_salary_encoder(train, ['companyId', 'jobType', 'degree', 'major', 'industry'])\n",
    "# train = train.drop(columns=['companyId', 'degree', 'major'])\n",
    "\n",
    "(100, ls, 8.000):\tMSE=354.008\n",
    "companyId  0.001198\n",
    "jobType  0.003756\n",
    "degree  0.054679\n",
    "major  0.041514\n",
    "industry  0.111630\n",
    "yearsExperience  0.000877\n",
    "milesFromMetropolis  0.001105\n",
    "JYM_mean  0.785242\n",
    "\n",
    "\n",
    "\n",
    "salary_dict = dict(train.groupby([feature]).salary.mean())\n",
    "# salary_dict = dict(np.floor(train.groupby([feature]).salary.mean()).rank())\n",
    "\n",
    "# train = group_k_columns(train, 3, ['jobType', 'yearsExperience', 'milesFromMetropolis'], 'JYM')\n",
    "train = mean_salary_encoder(train, ['companyId', 'jobType', 'degree', 'major', 'industry'])\n",
    "# train = train.drop(columns=['companyId', 'degree', 'major'])\n",
    "\n",
    "(100, ls, 8.000):\tMSE=357.078\n",
    "companyId  0.001164\n",
    "jobType  0.415428\n",
    "degree  0.111364\n",
    "major  0.038076\n",
    "industry  0.117292\n",
    "yearsExperience  0.186888\n",
    "milesFromMetropolis  0.129788\n",
    "\n",
    "\n",
    "\n",
    "salary_mean_dict = dict(train.groupby(col_names).salary.mean())\n",
    "salary_mean_dict['name'] = new_col_name_mean\n",
    "salary_lower_dict = dict(lower_bound)\n",
    "salary_lower_dict['name'] = new_col_name_Q0\n",
    "salary_Q1_dict = dict(Q1)\n",
    "salary_Q1_dict['name'] = new_col_name_Q1\n",
    "salary_Q2_dict = dict(train.groupby(col_names).salary.median())\n",
    "salary_Q2_dict['name'] = new_col_name_Q2\n",
    "salary_Q3_dict = dict(Q3)\n",
    "salary_Q3_dict['name'] = new_col_name_Q3\n",
    "salary_upper_dict = dict(upper_bound)\n",
    "salary_upper_dict['name'] = new_col_name_Q4\n",
    "salary_dictionaries = [salary_mean_dict, salary_lower_dict, salary_Q1_dict, salary_Q2_dict, salary_Q3_dict, salary_upper_dict]\n",
    "\n",
    "train = group_k_columns(train, 5, ['companyId', 'jobType', 'degree', 'major', 'industry'], 'CJDMI')\n",
    "train = mean_salary_encoder(train, ['companyId', 'jobType', 'degree', 'major', 'industry'])\n",
    "\n",
    "(40, ls, 8.000):\tMSE=321.275\n",
    "companyId  0.000038\n",
    "jobType  0.004394\n",
    "degree  0.001057\n",
    "major  0.001053\n",
    "industry  0.002115\n",
    "yearsExperience  0.153585\n",
    "milesFromMetropolis  0.105777\n",
    "CJDMI_mean  0.676035\n",
    "CJDMI_lower  0.002281\n",
    "CJDMI_Q1  0.034198\n",
    "CJDMI_median  0.007061\n",
    "CJDMI_Q3  0.010344\n",
    "CJDMI_upper  0.002063\n",
    "\n",
    "(100, ls, 8.000):\tMSE=318.217\n",
    "companyId  0.000241\n",
    "jobType  0.004977\n",
    "degree  0.001390\n",
    "major  0.001454\n",
    "industry  0.002724\n",
    "yearsExperience  0.153228\n",
    "milesFromMetropolis  0.106019\n",
    "CJDMI_mean  0.671498\n",
    "CJDMI_lower  0.002739\n",
    "CJDMI_Q1  0.034679\n",
    "CJDMI_median  0.007618\n",
    "CJDMI_Q3  0.010905\n",
    "CJDMI_upper  0.002526\n",
    "\n",
    "(150, ls, 8.000):\tMSE=318.538\n",
    "companyId  0.000416\n",
    "jobType  0.005046\n",
    "degree  0.001426\n",
    "major  0.001551\n",
    "industry  0.002871\n",
    "yearsExperience  0.153244\n",
    "milesFromMetropolis  0.106380\n",
    "CJDMI_mean  0.669683\n",
    "CJDMI_lower  0.002943\n",
    "CJDMI_Q1  0.034792\n",
    "CJDMI_median  0.007855\n",
    "CJDMI_Q3  0.011059\n",
    "CJDMI_upper  0.002733\n",
    "\n",
    "(200, ls, 8.000):\tMSE=319.019\n",
    "companyId  0.000576\n",
    "jobType  0.005077\n",
    "degree  0.001444\n",
    "major  0.001626\n",
    "industry  0.002970\n",
    "yearsExperience  0.153258\n",
    "milesFromMetropolis  0.106739\n",
    "CJDMI_mean  0.668141\n",
    "CJDMI_lower  0.003097\n",
    "CJDMI_Q1  0.034884\n",
    "CJDMI_median  0.008074\n",
    "CJDMI_Q3  0.011209\n",
    "CJDMI_upper  0.002904\n",
    "\n",
    "\n",
    "\n",
    "train = group_k_columns(train, 5, ['companyId', 'jobType', 'degree', 'major', 'industry'], 'CJDMI')\n",
    "# train = mean_salary_encoder(train, ['companyId', 'jobType', 'degree', 'major', 'industry'])\n",
    "train = train.drop(columns=['companyId', 'jobType', 'degree', 'major', 'industry'])\n",
    "\n",
    "(100, ls, 8.000):\tMSE=323.537\n",
    "yearsExperience  0.153815\n",
    "milesFromMetropolis  0.106334\n",
    "CJDMI_mean  0.680445\n",
    "CJDMI_lower  0.002935\n",
    "CJDMI_Q1  0.035180\n",
    "CJDMI_median  0.008045\n",
    "CJDMI_Q3  0.010879\n",
    "CJDMI_upper  0.002365\n",
    "\n",
    "\n",
    "\n",
    "salary_mean_dict = dict(train.groupby(col_names).salary.mean())\n",
    "salary_mean_dict['name'] = new_col_name_mean\n",
    "salary_min_dict = dict(train.groupby(col_names).salary.min())\n",
    "salary_min_dict['name'] = new_col_name_min\n",
    "salary_lower_dict = dict(lower_bound)\n",
    "salary_lower_dict['name'] = new_col_name_Q0\n",
    "salary_Q1_dict = dict(Q1)\n",
    "salary_Q1_dict['name'] = new_col_name_Q1\n",
    "salary_Q2_dict = dict(train.groupby(col_names).salary.median())\n",
    "salary_Q2_dict['name'] = new_col_name_Q2\n",
    "salary_Q3_dict = dict(Q3)\n",
    "salary_Q3_dict['name'] = new_col_name_Q3\n",
    "salary_upper_dict = dict(upper_bound)\n",
    "salary_upper_dict['name'] = new_col_name_Q4\n",
    "salary_max_dict = dict(train.groupby(col_names).salary.max())\n",
    "salary_max_dict['name'] = new_col_name_max\n",
    "salary_dictionaries = [salary_mean_dict, salary_min_dict, salary_lower_dict, salary_Q1_dict, \n",
    "                       salary_Q2_dict, salary_Q3_dict, salary_upper_dict, salary_max_dict]\n",
    "\n",
    "(100, ls, 8.000):\tMSE=313.483\n",
    "yearsExperience  0.152787\n",
    "milesFromMetropolis  0.105388\n",
    "CJDMI_mean  0.665288\n",
    "CJDMI_min  0.010648\n",
    "CJDMI_lower  0.001203\n",
    "CJDMI_Q1  0.031204\n",
    "CJDMI_median  0.007509\n",
    "CJDMI_Q3  0.007989\n",
    "CJDMI_upper  0.001416\n",
    "CJDMI_max  0.016567\n",
    "\n",
    "\n",
    "\n",
    "train = group_k_columns(train, 5, ['companyId', 'jobType', 'degree', 'major', 'industry'], 'CJDMI')\n",
    "train = mean_salary_encoder(train, ['companyId', 'jobType', 'degree', 'major', 'industry'])\n",
    "train = train.drop(columns=['companyId', 'jobType', 'degree', 'major', 'industry'])\n",
    "\n",
    "(100, ls, 8.000):\tMSE=313.491\n",
    "yearsExperience  0.152785\n",
    "milesFromMetropolis  0.105390\n",
    "CJDMI_mean  0.665287\n",
    "CJDMI_min  0.010648\n",
    "CJDMI_lower  0.001202\n",
    "CJDMI_Q1  0.031206\n",
    "CJDMI_median  0.007507\n",
    "CJDMI_Q3  0.007989\n",
    "CJDMI_upper  0.001419\n",
    "CJDMI_max  0.016567\n",
    "\n",
    "\n",
    "\n",
    "train = group_k_columns(train, 5, ['companyId', 'jobType', 'degree', 'major', 'industry'], 'CJDMI')\n",
    "train = mean_salary_encoder(train, ['companyId', 'jobType', 'degree', 'major', 'industry'])\n",
    "# train = train.drop(columns=['companyId', 'jobType', 'degree', 'major', 'industry'])\n",
    "\n",
    "(100, ls, 8.000):\tMSE=307.004\n",
    "companyId  0.000191\n",
    "jobType  0.003687\n",
    "degree  0.002976\n",
    "major  0.001054\n",
    "industry  0.002109\n",
    "yearsExperience  0.151975\n",
    "milesFromMetropolis  0.104943\n",
    "CJDMI_mean  0.658703\n",
    "CJDMI_min  0.008978\n",
    "CJDMI_lower  0.001002\n",
    "CJDMI_Q1  0.030981\n",
    "CJDMI_median  0.007029\n",
    "CJDMI_Q3  0.007857\n",
    "CJDMI_upper  0.001499\n",
    "CJDMI_max  0.017016\n",
    "\n",
    "\n",
    "\n",
    "salary_min_dict = dict(train.groupby(col_names).salary.min())\n",
    "salary_min_dict['name'] = new_col_name_min\n",
    "salary_Q1_dict = dict(Q1)\n",
    "salary_Q1_dict['name'] = new_col_name_Q1\n",
    "salary_mean_dict = dict(train.groupby(col_names).salary.mean())\n",
    "salary_mean_dict['name'] = new_col_name_mean\n",
    "salary_Q2_dict = dict(train.groupby(col_names).salary.median())\n",
    "salary_Q2_dict['name'] = new_col_name_Q2\n",
    "salary_Q3_dict = dict(Q3)\n",
    "salary_Q3_dict['name'] = new_col_name_Q3\n",
    "salary_upper_dict = dict(upper_bound)\n",
    "salary_upper_dict['name'] = new_col_name_Q4\n",
    "salary_max_dict = dict(train.groupby(col_names).salary.max())\n",
    "salary_max_dict['name'] = new_col_name_max\n",
    "salary_dictionaries = [salary_mean_dict, salary_min_dict, \"\"\"salary_lower_dict\"\"\", salary_Q1_dict, \n",
    "                       salary_Q2_dict, salary_Q3_dict, salary_upper_dict, salary_max_dict]\n",
    "\n",
    "(100, ls, 8.000):\tMSE=306.779\n",
    "companyId  0.000199\n",
    "jobType  0.003814\n",
    "degree  0.003028\n",
    "major  0.001074\n",
    "industry  0.002091\n",
    "yearsExperience  0.152010\n",
    "milesFromMetropolis  0.104899\n",
    "CJDMI_mean  0.658765\n",
    "CJDMI_min  0.009157\n",
    "CJDMI_Q1  0.031207\n",
    "CJDMI_median  0.006925\n",
    "CJDMI_Q3  0.007889\n",
    "CJDMI_upper  0.001902\n",
    "CJDMI_max  0.017041\n",
    "\n",
    "\n",
    "\n",
    "# salary_dict = dict(train.groupby([feature]).salary.mean())\n",
    "# salary_dict = dict(np.floor(train.groupby([feature]).salary.mean()).rank())\n",
    "salary_dict = {}\n",
    "for cat in train[feature].unique():\n",
    "    salary_dict[cat] = train[train[feature]==cat].salary.mode()[0]\n",
    "\n",
    "(100, ls, 8.000):\tMSE=307.692\n",
    "companyId  0.000168\n",
    "jobType  0.003588\n",
    "degree  0.002864\n",
    "major  0.001076\n",
    "industry  0.001801\n",
    "yearsExperience  0.152146\n",
    "milesFromMetropolis  0.105048\n",
    "CJDMI_mean  0.659155\n",
    "CJDMI_min  0.009254\n",
    "CJDMI_Q1  0.031239\n",
    "CJDMI_median  0.006950\n",
    "CJDMI_Q3  0.007850\n",
    "CJDMI_upper  0.001870\n",
    "CJDMI_max  0.016991\n",
    "\n",
    "\n",
    "\n",
    "salary_dictionaries = [salary_mean_dict, salary_min_dict, salary_Q1_dict, #salary_lower_dict\n",
    "                       salary_Q2_dict, salary_Q3_dict, salary_max_dict]#salary_upper_dict,\n",
    "\n",
    "(100, ls, 8.000):\tMSE=307.603\n",
    "companyId  0.000186\n",
    "jobType  0.003569\n",
    "degree  0.002906\n",
    "major  0.001058\n",
    "industry  0.001777\n",
    "yearsExperience  0.152123\n",
    "milesFromMetropolis  0.105045\n",
    "CJDMI_mean  0.659153\n",
    "CJDMI_min  0.009398\n",
    "CJDMI_Q1  0.031698\n",
    "CJDMI_median  0.006880\n",
    "CJDMI_Q3  0.008998\n",
    "CJDMI_max  0.017209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [## Boosting\n",
    "              ('AB', AdaBoostRegressor()), \n",
    "              ('GBR', GradientBoostingRegressor()), \n",
    "              ## Bagging\n",
    "              ('RF', RandomForestRegressor()), \n",
    "              ('ET', ExtraTreesRegressor())]\n",
    "\n",
    "for name, model in model_list:\n",
    "    model.fit(train_df, train_salary)\n",
    "    validation_pred = model.predict(validation_features)\n",
    "    mse = mean_squared_error(validation_salary, validation_pred)\n",
    "    mae = mean_absolute_error(validation_salary, validation_pred)\n",
    "    print(\"%s:\\tMSE=%.3f\\tMAE=%.3f\" % (name, mse, mae))\n",
    "\n",
    "# expect:\n",
    "# AB:\tMSE=555.790\tMAE=19.869\n",
    "# GBR:\tMSE=364.269\tMAE=15.487\n",
    "# RF:\tMSE=139.587\tMAE=9.053\n",
    "# ET:\tMSE=107.677\tMAE=6.012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = []\n",
    "# Boosting\n",
    "ensembles.append(('scaledAB',\n",
    "                  Pipeline([('Scaler', StandardScaler()),\n",
    "                            ('AB', AdaBoostRegressor())])))\n",
    "ensembles.append(('scaledGBR',\n",
    "                  Pipeline([('Scaler', StandardScaler()),\n",
    "                            ('GBR', GradientBoostingRegressor())])))\n",
    "## Bagging\n",
    "ensembles.append(('scaledRF',\n",
    "                  Pipeline([('Scaler', StandardScaler()),\n",
    "                            ('RF', RandomForestRegressor())])))\n",
    "ensembles.append(('scaledET',\n",
    "                  Pipeline([('Scaler', StandardScaler()),\n",
    "                            ('ET', ExtraTreesRegressor())])))\n",
    "\n",
    "for name, model in ensembles:\n",
    "    model.fit(train_df, train_salary)\n",
    "    validation_pred = model.predict(validation_features)\n",
    "    mse = mean_squared_error(validation_salary, validation_pred)\n",
    "    mae = mean_absolute_error(validation_salary, validation_pred)\n",
    "    print(\"%s:\\tMSE=%.3f\\tMAE=%.3f\" % (name, mse, mae))\n",
    "\n",
    "# expect:\n",
    "# scaledAB:\tMSE=553.240\tMAE=19.786\n",
    "# scaledGBR:\tMSE=364.269\tMAE=15.487\n",
    "# scaledRF:\tMSE=139.061\tMAE=9.046\n",
    "# scaledET:\tMSE=107.677\tMAE=6.012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = data.copy()\n",
    "model_list = [('LR', LinearRegression()), \n",
    "              ('LASSO', Lasso()), \n",
    "              ('EN', ElasticNet()), \n",
    "              ('CART', DecisionTreeRegressor()), \n",
    "              ('RF', RandomForestRegressor())]\n",
    "validation_rows = 10000\n",
    "for (model_name, model) in model_list:\n",
    "    regression.score = sklearn.model_selection.cross_val_score(model, train.iloc[:,:-1], train.iloc[:,-1], scoring=\"neg_mean_squared_error\")\n",
    "    print(model_name, -1.0 * regression.score.mean())\n",
    "\n",
    "#     model.fit(train_f, train_s)\n",
    "#     valid_pred = model.predict(valid_f)\n",
    "#     mse = mean_squared_error(valid_s, valid_pred)\n",
    "#     mae = mean_absolute_error(valid_s, valid_pred)\n",
    "#     print(\"model=%s,\\tto_drop_companyId=%s, to_merge_degree_major=%s:\\tMSE=%.3f\\tMAE=%.3f\" % \n",
    "#           (model_name, companyId, degree_major, mse, mae))\n",
    "\"\"\"\n",
    "model=LR,\tto_drop_companyId=False, to_merge_degree_major=False:\tMSE=393.114\tMAE=15.979\n",
    "model=LASSO,\tto_drop_companyId=False, to_merge_degree_major=False:\tMSE=394.213\tMAE=16.006\n",
    "model=EN,\tto_drop_companyId=False, to_merge_degree_major=False:\tMSE=400.965\tMAE=16.166\n",
    "model=CART,\tto_drop_companyId=False, to_merge_degree_major=False:\tMSE=686.648\tMAE=20.350\n",
    "model=RF,\tto_drop_companyId=False, to_merge_degree_major=False:\tMSE=435.528\tMAE=16.634\n",
    "model=LR,\tto_drop_companyId=True, to_merge_degree_major=False:\tMSE=393.119\tMAE=15.980\n",
    "model=LASSO,\tto_drop_companyId=True, to_merge_degree_major=False:\tMSE=394.216\tMAE=16.006\n",
    "model=EN,\tto_drop_companyId=True, to_merge_degree_major=False:\tMSE=400.967\tMAE=16.166\n",
    "model=CART,\tto_drop_companyId=True, to_merge_degree_major=False:\tMSE=642.333\tMAE=19.651\n",
    "model=RF,\tto_drop_companyId=True, to_merge_degree_major=False:\tMSE=438.890\tMAE=16.668\n",
    "model=LR,\tto_drop_companyId=False, to_merge_degree_major=True:\tMSE=958.742\tMAE=23.763\n",
    "model=LASSO,\tto_drop_companyId=False, to_merge_degree_major=True:\tMSE=948.846\tMAE=23.606\n",
    "model=EN,\tto_drop_companyId=False, to_merge_degree_major=True:\tMSE=1111.485\tMAE=25.481\n",
    "model=CART,\tto_drop_companyId=False, to_merge_degree_major=True:\tMSE=8083.295\tMAE=82.648\n",
    "model=RF,\tto_drop_companyId=False, to_merge_degree_major=True:\tMSE=7966.888\tMAE=82.662\n",
    "model=LR,\tto_drop_companyId=True, to_merge_degree_major=True:\tMSE=958.884\tMAE=23.764\n",
    "model=LASSO,\tto_drop_companyId=True, to_merge_degree_major=True:\tMSE=948.890\tMAE=23.607\n",
    "model=EN,\tto_drop_companyId=True, to_merge_degree_major=True:\tMSE=1111.605\tMAE=25.482\n",
    "model=CART,\tto_drop_companyId=True, to_merge_degree_major=True:\tMSE=8006.305\tMAE=82.538\n",
    "model=RF,\tto_drop_companyId=True, to_merge_degree_major=True:\tMSE=7948.391\tMAE=82.554\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data.copy()\n",
    "test_list = [(True, False)]\n",
    "model_list = [## Boosting\n",
    "#               ('AB', AdaBoostRegressor()), \n",
    "              ('GBR', GradientBoostingRegressor(n_estimators=40, max_depth=7, loss='ls')), \n",
    "              ## Bagging\n",
    "              ('RF', RandomForestRegressor(n_estimators=60, max_depth=15, min_samples_split=80, max_features=5)), ]\n",
    "#               ('ET', ExtraTreesRegressor())]\n",
    "validation_rows = 10000\n",
    "for (companyId, degree_major) in test_list:\n",
    "    for (model_name, model) in model_list:\n",
    "        train = feature_engineer(to_drop_companyId=companyId, to_merge_degree_major=degree_major, train=data.copy())\n",
    "\n",
    "        valid_f = train.iloc[:validation_rows, :-1]\n",
    "        valid_s = train.iloc[:validation_rows, -1].values.astype(float)\n",
    "        train_f = train.drop(index=train.index[:validation_rows])\n",
    "        train_s = train_f.pop('salary')\n",
    "\n",
    "        model.fit(train_f, train_s)\n",
    "        valid_pred = model.predict(valid_f)\n",
    "        mse = mean_squared_error(valid_s, valid_pred)\n",
    "        mae = mean_absolute_error(valid_s, valid_pred)\n",
    "        print(\"model=%s,\\tto_drop_companyId=%s, to_merge_degree_major=%s:\\tMSE=%.3f\\tMAE=%.3f\" % \n",
    "              (model_name, companyId, degree_major, mse, mae))\n",
    "\n",
    "\"\"\"\n",
    "Expect:\n",
    "model=AB,\tto_drop_companyId=False, to_merge_degree_major=False:\tMSE=545.306\tMAE=19.586\n",
    "model=GBR,\tto_drop_companyId=False, to_merge_degree_major=False:\tMSE=366.419\tMAE=15.532\n",
    "model=RF,\tto_drop_companyId=False, to_merge_degree_major=False:\tMSE=434.414\tMAE=16.592\n",
    "model=ET,\tto_drop_companyId=False, to_merge_degree_major=False:\tMSE=507.821\tMAE=17.748\n",
    "model=AB,\tto_drop_companyId=True, to_merge_degree_major=False:\tMSE=555.479\tMAE=19.755\n",
    "model=GBR,\tto_drop_companyId=True, to_merge_degree_major=False:\tMSE=366.419\tMAE=15.532\n",
    "model=RF,\tto_drop_companyId=True, to_merge_degree_major=False:\tMSE=436.426\tMAE=16.643\n",
    "model=ET,\tto_drop_companyId=True, to_merge_degree_major=False:\tMSE=507.160\tMAE=17.782\n",
    "model=AB,\tto_drop_companyId=False, to_merge_degree_major=True:\tMSE=10058.697\tMAE=96.473\n",
    "model=GBR,\tto_drop_companyId=False, to_merge_degree_major=True:\tMSE=7936.804\tMAE=83.056\n",
    "model=RF,\tto_drop_companyId=False, to_merge_degree_major=True:\tMSE=7964.971\tMAE=82.636\n",
    "model=ET,\tto_drop_companyId=False, to_merge_degree_major=True:\tMSE=8044.035\tMAE=82.660\n",
    "model=AB,\tto_drop_companyId=True, to_merge_degree_major=True:\tMSE=10134.425\tMAE=96.792\n",
    "model=GBR,\tto_drop_companyId=True, to_merge_degree_major=True:\tMSE=7936.804\tMAE=83.056\n",
    "model=RF,\tto_drop_companyId=True, to_merge_degree_major=True:\tMSE=7949.010\tMAE=82.555\n",
    "model=ET,\tto_drop_companyId=True, to_merge_degree_major=True:\tMSE=7988.281\tMAE=82.480\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
